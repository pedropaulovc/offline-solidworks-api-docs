# Offline SolidWorks API Documentation

A comprehensive crawler and transformation pipeline for creating offline, searchable versions of the SolidWorks API documentation with XMLDoc files for Visual Studio IntelliSense.

## üöÄ Quick Start: Using Pre-Generated XMLDoc Files

**Want IntelliSense for SolidWorks API?** Just follow these steps:

1. **Download** `SolidWorks.Interop.xmldoc.v1.0.0.zip` from the [latest release](https://github.com/pedropaulovc/offline-solidworks-api-docs/releases/latest)
2. **Extract** the XML files from the archive
3. **Copy** them to the same folder where the SolidWorks SDK DLLs are located:
   - **SOLIDWORKS 3DEXPERIENCE**: `C:\Program Files\Dassault Systemes\SOLIDWORKS 3DEXPERIENCE\SOLIDWORKS\api\redist`
   - **Standard SOLIDWORKS**: `C:\Program Files\SOLIDWORKS Corp\SOLIDWORKS\api\redist`
4. **Restart** Visual Studio or your IDE
5. **Enjoy** full IntelliSense documentation with tooltips, parameter info, and examples!

The XML files should be placed alongside their corresponding DLL files (e.g., `SolidWorks.Interop.sldworks.xml` next to `SolidWorks.Interop.sldworks.dll`).

## üìã Overview

This project provides a reproducible pipeline for generating XMLDoc files that enable IntelliSense support for the SolidWorks API in Visual Studio and other IDEs.

### Pipeline Phases

The complete pipeline consists of 9 phases:

1. **Crawling** the SolidWorks API documentation using the expandToc API ‚úÖ
2. **Extracting** type information from the table of contents ‚úÖ
3. **Crawling** type member pages (properties and methods) ‚úÖ
4. **Extracting** type-level documentation (descriptions, examples, remarks) ‚úÖ
5. **Extracting** type member details (parameters, return values, remarks) ‚úÖ
6. **Extracting** enumeration members and values ‚úÖ
7. **Crawling** example pages ‚úÖ
8. **Parsing** example content into structured format ‚úÖ
9. **Generating** XMLDoc for Visual Studio IntelliSense ‚úÖ

## üèóÔ∏è Project Structure

```
offline-solidworks-api-docs/
‚îú‚îÄ‚îÄ 10_crawl_toc_pages/          # Phase 1: Crawl API docs via expandToc API
‚îÇ   ‚îú‚îÄ‚îÄ solidworks_scraper/      # Scrapy project
‚îÇ   ‚îú‚îÄ‚îÄ tests/                   # Test suite
‚îÇ   ‚îú‚îÄ‚îÄ output/                  # Crawled data (gitignored)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ html/                # HTML files and JSON TOC structure
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metadata/            # Crawl metadata (tracked)
‚îÇ   ‚îú‚îÄ‚îÄ run_crawler.py           # Main entry point
‚îÇ   ‚îî‚îÄ‚îÄ validate_crawl.py        # Validation script
‚îú‚îÄ‚îÄ 20_extract_types/            # Phase 2: Extract types from TOC
‚îÇ   ‚îú‚îÄ‚îÄ extract_types.py         # Main extraction script
‚îÇ   ‚îú‚îÄ‚îÄ validate_extraction.py   # Validation script
‚îÇ   ‚îú‚îÄ‚îÄ metadata/                # Output (api_types.xml)
‚îÇ   ‚îî‚îÄ‚îÄ tests/                   # Test suite
‚îú‚îÄ‚îÄ 30_crawl_type_members/       # Phase 3: Crawl member pages
‚îÇ   ‚îú‚îÄ‚îÄ solidworks_scraper/      # Scrapy project
‚îÇ   ‚îú‚îÄ‚îÄ tests/                   # Test suite
‚îÇ   ‚îú‚îÄ‚îÄ output/                  # Crawled HTML (gitignored)
‚îÇ   ‚îî‚îÄ‚îÄ run_crawler.py           # Main entry point
‚îú‚îÄ‚îÄ 40_extract_type_details/     # Phase 4: Extract type details
‚îÇ   ‚îú‚îÄ‚îÄ extract_type_info.py     # Main extraction script
‚îÇ   ‚îú‚îÄ‚îÄ validate_extraction.py   # Validation script
‚îÇ   ‚îú‚îÄ‚îÄ metadata/                # Output (api_types.xml)
‚îÇ   ‚îî‚îÄ‚îÄ tests/                   # Test suite
‚îú‚îÄ‚îÄ 50_extract_type_member_details/ # Phase 5: Extract member details
‚îÇ   ‚îú‚îÄ‚îÄ extract_member_details.py # Main extraction script
‚îÇ   ‚îú‚îÄ‚îÄ validate_extraction.py   # Validation script
‚îÇ   ‚îú‚îÄ‚îÄ output/                  # Output (member_details.xml)
‚îÇ   ‚îî‚îÄ‚îÄ tests/                   # Test suite
‚îú‚îÄ‚îÄ 60_extract_enum_members/     # Phase 6: Extract enumeration members
‚îÇ   ‚îú‚îÄ‚îÄ extract_enum_members.py  # Main extraction script
‚îÇ   ‚îú‚îÄ‚îÄ metadata/                # Output (enum_members.xml)
‚îÇ   ‚îî‚îÄ‚îÄ tests/                   # Test suite
‚îú‚îÄ‚îÄ 70_crawl_examples/           # Phase 7: Crawl example pages
‚îÇ   ‚îú‚îÄ‚îÄ solidworks_scraper/      # Scrapy project
‚îÇ   ‚îú‚îÄ‚îÄ output/                  # Crawled HTML (gitignored)
‚îÇ   ‚îî‚îÄ‚îÄ run_crawler.py           # Main entry point
‚îú‚îÄ‚îÄ 80_parse_examples/           # Phase 8: Parse example content
‚îÇ   ‚îú‚îÄ‚îÄ parse_examples.py        # Main parsing script
‚îÇ   ‚îú‚îÄ‚îÄ validate_parse.py        # Validation script
‚îÇ   ‚îú‚îÄ‚îÄ output/                  # Output (examples.xml)
‚îÇ   ‚îî‚îÄ‚îÄ tests/                   # Test suite
‚îú‚îÄ‚îÄ 90_generate_xmldoc/          # Phase 9: Generate XMLDoc files
‚îÇ   ‚îú‚îÄ‚îÄ generate_xmldoc.py       # Main generation script
‚îÇ   ‚îú‚îÄ‚îÄ data_merger.py           # Data merging utilities
‚îÇ   ‚îú‚îÄ‚îÄ xmldoc_id.py             # XMLDoc ID generation
‚îÇ   ‚îú‚îÄ‚îÄ output/                  # XMLDoc files
‚îÇ   ‚îî‚îÄ‚îÄ tests/                   # Test suite
‚îú‚îÄ‚îÄ 100_crawl_programming_guide/ # Phase 10: Crawl programming guide
‚îú‚îÄ‚îÄ 110_extract_docs_md/         # Phase 11: Extract docs to Markdown
‚îú‚îÄ‚îÄ shared/                      # Shared utilities and helpers
‚îú‚îÄ‚îÄ CLAUDE.md                    # Project context for AI assistants
‚îú‚îÄ‚îÄ pyproject.toml               # Python project configuration
‚îî‚îÄ‚îÄ README.md                    # This file
```

## üöÄ Quick Start

### Prerequisites

- Python 3.12+
- uv package manager

### Installation

```bash
# Clone the repository
git clone https://github.com/pedropaulovc/offline-solidworks-api-docs.git
cd offline-solidworks-api-docs

# Install dependencies with uv
uv sync
```

### Running the Complete Pipeline

To generate the XMLDoc files yourself (instead of using the pre-generated ones):

#### Phase 1: Crawl Documentation

```bash
# Run a test crawl (limited pages)
uv run python 10_crawl_toc_pages/run_crawler.py --test

# Run full crawl (will take several hours)
uv run python 10_crawl_toc_pages/run_crawler.py

# Resume interrupted crawl
uv run python 10_crawl_toc_pages/run_crawler.py --resume

# Validate crawl results
uv run python 10_crawl_toc_pages/validate_crawl.py
```

#### Phase 2: Extract Types

```bash
# Extract types from table of contents
uv run python 20_extract_types/extract_types.py

# Validate extraction results
uv run python 20_extract_types/validate_extraction.py
```

#### Phase 3: Crawl Type Members

```bash
# Crawl member pages (properties and methods)
uv run python 30_crawl_type_members/run_crawler.py
```

#### Phase 4: Extract Type Details

```bash
# Extract type descriptions, examples, and remarks
uv run python 40_extract_type_details/extract_type_info.py

# Validate extraction results
uv run python 40_extract_type_details/validate_extraction.py
```

#### Phase 5: Extract Member Details

```bash
# Extract member parameters, return values, and remarks
uv run python 50_extract_type_member_details/extract_member_details.py

# Validate extraction results
uv run python 50_extract_type_member_details/validate_extraction.py
```

#### Phase 6: Extract Enum Members

```bash
# Extract enumeration members and values
uv run python 60_extract_enum_members/extract_enum_members.py
```

#### Phase 7: Crawl Examples

```bash
# Crawl example pages
uv run python 70_crawl_examples/run_crawler.py
```

#### Phase 8: Parse Examples

```bash
# Parse example content into structured format
uv run python 80_parse_examples/parse_examples.py

# Validate parsing results
uv run python 80_parse_examples/validate_parse.py
```

#### Phase 9: Generate XMLDoc

```bash
# Generate XMLDoc files for IntelliSense
uv run python 90_generate_xmldoc/generate_xmldoc.py

# Validate XMLDoc generation
uv run python 90_generate_xmldoc/validate_generation.py
```

## üìä Expected Results

### Phase 1: Crawling
- **460+ pages** from the SolidWorks API documentation
- Clean HTML content extracted from `__NEXT_DATA__` JSON
- JSON TOC structure from expandToc API
- Complete metadata for reproducibility
- **>95% success rate** for validation to pass

### Phase 2: Type Extraction
- **458+ types** with member information
- Properties and methods extracted from table of contents
- Output: `api_types.xml`

### Phase 3: Member Crawling
- **~11,500+ member pages** crawled (properties and methods)
- HTML content for each type member
- Complete metadata for reproducibility

### Phase 4: Type Details Extraction
- **1674+ type files** processed
- **~1568 types** with descriptions (93%)
- **~561 types** with code examples (33%)
- **~3708 total examples** (VBA, VB.NET, C#, C++)
- **~455 types** with remarks (27%)
- Output: `api_types.xml` (~5 MB)

### Phase 5: Member Details Extraction
- **~11,523 members** extracted
- Parameter details, return values, and remarks
- Output: `member_details.xml`

### Phase 6: Enum Extraction
- Enumeration members and values extracted
- Output: `enum_members.xml`

### Phase 7: Example Crawling
- Example pages crawled from documentation
- HTML content for code examples

### Phase 8: Example Parsing
- Code examples extracted and parsed
- Indentation preserved with CDATA wrapping
- Output: `examples.xml`

### Phase 9: XMLDoc Generation
- **10 XMLDoc files** generated (one per assembly)
- Complete IntelliSense documentation
- Output: `90_generate_xmldoc/output/*.xml`

## üß™ Testing

```bash
# Run all tests
uv run pytest -v

# Run tests for specific phase
uv run pytest 10_crawl_toc_pages/tests/ -v
uv run pytest 20_extract_types/tests/ -v
uv run pytest 40_extract_type_details/tests/ -v
uv run pytest 50_extract_type_member_details/tests/ -v
uv run pytest 60_extract_enum_members/tests/ -v
uv run pytest 80_parse_examples/tests/ -v
uv run pytest 90_generate_xmldoc/tests/ -v

# Run with coverage
uv run pytest --cov --cov-report=html
```

## üìù Output Formats

### Phase 1: Crawl Metadata (JSON)

**urls_crawled.jsonl** - One record per crawled page:
```json
{
  "url": "https://help.solidworks.com/2026/english/api/sldworksapi/...",
  "timestamp": "2025-11-14T10:30:00Z",
  "file_path": "output/html/sldworksapi/...",
  "content_hash": "sha256:abc123...",
  "content_length": 12345,
  "status_code": 200,
  "title": "IAdvancedHoleFeatureData Interface",
  "session_id": "2025-11-14-103000"
}
```

**crawl_stats.json** - Summary statistics:
```json
{
  "start_time": "2025-11-14T10:00:00Z",
  "end_time": "2025-11-14T12:00:00Z",
  "total_pages": 460,
  "successful_pages": 458,
  "failed_pages": 2,
  "skipped_pages": 0
}
```

### Phases 2-9: Extracted Data (XML)

All extraction phases produce structured XML files:
- **Phase 2**: `api_types.xml` - Type definitions
- **Phase 4**: `api_types.xml` - Type descriptions, examples, and remarks
- **Phase 5**: `member_details.xml` - Member parameters, return values, and remarks
- **Phase 6**: `enum_members.xml` - Enumeration members and values
- **Phase 8**: `examples.xml` - Code examples in structured format
- **Phase 9**: `SolidWorks.Interop.*.xml` - XMLDoc files for IntelliSense (10 files)

Each extraction also produces a summary JSON file with statistics.

## üîß Configuration

### Crawler Settings

Key settings in `10_crawl_toc_pages/solidworks_scraper/settings.py`:

- **User Agent**: Chrome browser to ensure proper access
- **Crawl Delay**: 2 seconds (respectful crawling)
- **Concurrent Requests**: 1 (polite single-threaded)
- **URL Boundary**: `/2026/english/api/` (stays within API docs)
- **Robots.txt**: Respected by default

### Python Project

Configuration in `pyproject.toml`:

- **Python Version**: 3.12+
- **Dependencies**: Scrapy, pytest, mypy, ruff, jsonlines
- **Code Quality**: Type checking with mypy, linting with ruff
- **Testing**: pytest with coverage support

## üö¶ Validation

Each phase includes validation scripts that check:

### Phase 1: Crawl Validation
- ‚úÖ Minimum page count (95% of expected 460)
- ‚úÖ All metadata files present and valid
- ‚úÖ HTML files match metadata records
- ‚úÖ No excessive duplicates
- ‚úÖ Success rate >95%

### Phases 2-9: Extraction Validation
- ‚úÖ XML files are well-formed
- ‚úÖ All required fields present
- ‚úÖ Type/member counts match expectations
- ‚úÖ Summary statistics are accurate

## üõ†Ô∏è Development

### Project Principles

1. **Modularity**: Each phase reads from previous, writes to next
2. **Reproducibility**: All outputs are deterministic
3. **Metadata-Driven**: Comprehensive tracking for validation
4. **Copyright Compliance**: HTML content gitignored, users crawl themselves

### Code Quality

```bash
# Type checking with mypy
uv run mypy 10_crawl_toc_pages/
uv run mypy 20_extract_types/
uv run mypy 40_extract_type_details/

# Linting with ruff
uv run ruff check .

# Auto-fix linting issues
uv run ruff check --fix .

# Run pre-commit hooks
uv run pre-commit run --all-files
```

## üìÑ License

The crawler code is provided for educational purposes. The SolidWorks API documentation content is copyrighted by Dassault Syst√®mes SolidWorks Corporation.

## ü§ù Contributing

Contributions are welcome for:
- Improving crawler reliability
- Adding transformation pipelines
- Enhancing validation
- Documentation improvements

Please ensure all contributions maintain reproducibility and respect copyright.

## ‚ö° Performance

### Complete Pipeline
- **Total Time**: ~4-5 hours for complete pipeline
- **Storage**: ~500-600 MB (HTML + intermediate files)
- **Final Output**: 10 XMLDoc files (~2 MB compressed)

### Phase-by-Phase Breakdown
- **Phase 1** (Crawl TOC): ~15 minutes, ~150 MB HTML
- **Phase 2** (Extract Types): ~10 seconds
- **Phase 3** (Crawl Members): ~3-4 hours, ~400 MB HTML
- **Phase 4** (Extract Type Details): ~30 seconds
- **Phase 5** (Extract Member Details): ~60 seconds
- **Phase 6** (Extract Enums): ~10 seconds
- **Phase 7** (Crawl Examples): ~10 minutes
- **Phase 8** (Parse Examples): ~20 seconds
- **Phase 9** (Generate XMLDoc): ~30 seconds

## üêõ Troubleshooting

### Phase 1: Crawling Issues

1. **"scrapy: command not found"**
   - Use `uv run` prefix: `uv run python 10_crawl_toc_pages/run_crawler.py`

2. **Rate limiting or 403 errors**
   - Increase DOWNLOAD_DELAY in settings.py
   - Check robots.txt compliance

3. **Incomplete crawl**
   - Use `--resume` flag to continue
   - Check errors.jsonl for failed URLs

4. **No pages discovered**
   - Check expandToc API is accessible
   - Verify starting URL returns valid JSON

### Phases 2-9: Extraction Issues

1. **"No HTML files found"**
   - Ensure Phase 1 crawl has completed successfully
   - Check input directory path is correct

2. **Validation failures**
   - Run with `--verbose` for details
   - Check extraction_summary.json for error patterns

3. **Missing type information**
   - Normal - not all types have examples/remarks
   - Check validation percentages for expected coverage

## üìö Documentation Structure

Each phase has detailed documentation:

- **README.md** (this file) - Project overview and quick start
- **10_crawl_toc_pages/README.md** - Crawler implementation details
- **20_extract_types/README.md** - Type extraction details
- **30_crawl_type_members/README.md** - Member crawling details
- **40_extract_type_details/README.md** - Type detail extraction
- **50_extract_type_member_details/README.md** - Member detail extraction
- **60_extract_enum_members/README.md** - Enum extraction details
- **70_crawl_examples/README.md** - Example crawling details
- **80_parse_examples/README.md** - Example parsing details
- **90_generate_xmldoc/README.md** - XMLDoc generation details
- **100_crawl_programming_guide/README.md** - Programming guide crawling
- **110_extract_docs_md/README.md** - Markdown extraction details
- **CLAUDE.md** - Project context for AI assistants

## üîÆ Future Enhancements (Planned)

- **Phase 120**: Create searchable offline documentation browser
- **Phase 130**: Export to additional formats (JSON, PDF)
- **Enhanced search**: Full-text search across all documentation
- **IDE plugins**: Direct integration with Visual Studio, VS Code, etc.

## ‚ö†Ô∏è Legal Notice

**IMPORTANT**: This tool is designed for personal, educational, and fair use only. The crawled documentation content is copyrighted by Dassault Syst√®mes SolidWorks Corporation.

- ‚ùå DO NOT redistribute crawled content
- ‚ùå DO NOT use for commercial purposes
- ‚úÖ Each user must crawl the documentation themselves
- ‚úÖ Pre-generated XMLDoc files can be shared (fair use)
- üìÅ Crawled HTML content is gitignored and not included in this repository

**Remember**: Always respect copyright and use this tool responsibly for personal/educational purposes only.
